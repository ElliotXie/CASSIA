# -*- coding: utf-8 -*-
"""Untitled44.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KVHuNGORcgfvLhu-BHjBfEzkZbNjgfOG
"""

# Commented out IPython magic to ensure Python compatibility.
# Install required packages
!pip install llama-index anthropic sentence-transformers faiss-cpu
!pip install llama-index-llms-anthropic
# %pip install llama-index-embeddings-huggingface
# %pip install llama-index-embeddings-instructor
# %pip install llama-index-vector-stores-faiss

from google.colab import drive
drive.mount('/content/drive')
data_dir = "/content/drive/MyDrive/rag_papers"

import os
from typing import List
import pandas as pd
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    ServiceContext,
    Settings,
    Document,
    StorageContext
)
from llama_index.llms.anthropic import Anthropic
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.node_parser import SentenceWindowNodeParser
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor
import faiss

# Set your Anthropic API key
os.environ['ANTHROPIC_API_KEY'] = "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"
llm_model = "claude-3-5-sonnet-20240620" #claude-3-sonnet-20240229
embed_model = "sentence-transformers/all-MiniLM-L6-v2"
context_window = 4096
embedding_dim = 384
similarity_top_k=8
max_output_tokens = 1024

def load_pdf_documents(pdf_dir):
    print("now process pdf files")
    reader = SimpleDirectoryReader(input_dir=pdf_dir, file_extractor=None)
    documents = reader.load_data()
    return documents

def convert_excel_csv_to_documents(input_dir: str) -> List[Document]:
    documents = []
    for filename in os.listdir(input_dir):
        file_path = os.path.join(input_dir, filename)
        if filename.lower().endswith(('.xlsx', '.xls')):
            try:
                excel_file = pd.ExcelFile(file_path)
                sheets = excel_file.sheet_names
                print(f"Processing Excel file: {filename}, Sheets: {sheets}")

                excel_content = []
                for sheet in sheets:
                    df = pd.read_excel(file_path, sheet_name=sheet)
                    if df.empty:
                        print(f"Sheet '{sheet}' in {filename} is empty. Skipping.")
                        continue
                    df.fillna('N/A', inplace=True)

                    sheet_content = f"Sheet: {sheet}\n"
                    sheet_content += df.to_string(index=False)
                    excel_content.append(sheet_content)

                full_content = f"File: {filename}\n\n" + "\n\n".join(excel_content)
                documents.append(Document(text=full_content))

            except Exception as e:
                print(f"Error processing Excel file {filename}: {e}")

        elif filename.lower().endswith('.csv'):
            try:
                df = pd.read_csv(file_path)
                if df.empty:
                    print(f"CSV file {filename} is empty. Skipping.")
                    continue
                df.fillna('N/A', inplace=True)
                print(f"Processing CSV file: {filename}, Rows: {len(df)}")

                csv_content = f"File: {filename}\n\n"
                csv_content += df.to_string(index=False)
                documents.append(Document(text=csv_content))

            except Exception as e:
                print(f"Error processing CSV file {filename}: {e}")

        else:
            print(f"Unsupported file format for file: {filename}. Skipping.")

    print(f"Total Documents Created: {len(documents)}")
    return documents

# Configuration
Settings.llm = Anthropic(
    model= llm_model,  # or another appropriate Claude model
    temperature=0.0,
    max_tokens=1024
)
Settings.llm = Anthropic(
    model="claude-3-5-sonnet-20240620",  # or another appropriate Claude model
    temperature=0.0,
    max_tokens=1024
)

Settings.embed_model = HuggingFaceEmbedding(
    model_name = embed_model
)

Settings.text_splitter = SentenceSplitter(
    separator=" ",
    chunk_size=1024,
    chunk_overlap=20,
    paragraph_separator="\n\n\n",
    secondary_chunking_regex="[^,.; ]+[,.; ]?",
)

Settings.context_window = context_window
Settings.num_output = min(max_output_tokens, context_window // 2)

Settings.node_parser = SentenceWindowNodeParser.from_defaults(
    window_size=3,
    window_metadata_key="window",
    original_text_metadata_key="original_text"
)


faiss_index = faiss.IndexFlatL2(embedding_dim)
vector_store = FaissVectorStore(faiss_index)

storage_context = StorageContext.from_defaults(vector_store=vector_store)

csv_documents = convert_excel_csv_to_documents(data_dir)
pdf_documents = load_pdf_documents(data_dir)
all_documents = csv_documents + pdf_documents

index = VectorStoreIndex.from_documents(
    all_documents,
    storage_context=storage_context
)

retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=similarity_top_k,
    vector_store_query_mode="default"
)

# Create a custom query engine
query_engine = RetrieverQueryEngine.from_args(
    retriever,
    node_postprocessors=[
        SimilarityPostprocessor(similarity_cutoff=0.65)
    ],
    streaming=True
)

# Example query
query = "markers for pro-B cell, pre-B cell, their similarity? and markers unique for pre-B? marker unique for pro-B? Answer each question carefully"

# Get response
response = query_engine.query(query)
print(response)

# Another example query
query = "markers for SSC in colon, detailly list out all of them. key words to find them might be something similar to: marker, marker gene, upregulate, downregulate, overexpress, underexpress, overrepresent, dominant, don't just restrict to genes, also include regulon, transcript factors, if it is regulon and transcript factors, list the main group name, and if later in the paper, there are detail discussion of which gene in that group, also list out."

response = query_engine.query(query)
print(response)