{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# CASSIA Local Testing Tutorial\n",
        "\n",
        "This notebook demonstrates how to use CASSIA locally without installing the package. It imports modules directly from the source code and includes basic functionality tests.\n",
        "\n",
        "## Cell Types in Example Dataset\n",
        "We'll analyze an intestinal cell dataset containing six distinct populations:\n",
        "1. monocyte\n",
        "2. plasma cell  \n",
        "3. cd8-positive, alpha-beta t cell\n",
        "4. transit amplifying cell of large intestine\n",
        "5. intestinal enteroendocrine cell\n",
        "6. intestinal crypt stem cell\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Import Local Modules\n",
        "\n",
        "First, let's import the required packages and local CASSIA modules:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully imported unified annotation boost modules\n"
          ]
        }
      ],
      "source": [
        "# Add current directory to path for proper imports\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "# Direct imports from local files, not from the installed package\n",
        "try:\n",
        "    from .tools_function import *\n",
        "except ImportError:\n",
        "    from tools_function import *\n",
        "try:\n",
        "    from .main_function_code import *\n",
        "except ImportError:\n",
        "    from main_function_code import *\n",
        "try:\n",
        "    from .Uncertainty_quantification import *\n",
        "except ImportError:\n",
        "    from Uncertainty_quantification import *\n",
        "try:\n",
        "    from .subclustering import *\n",
        "except ImportError:\n",
        "    from subclustering import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "import re\n",
        "\n",
        "# Import the new unified modules for annotation boost\n",
        "try:\n",
        "    from annotation_boost import (\n",
        "        iterative_marker_analysis,\n",
        "        runCASSIA_annotationboost,\n",
        "        runCASSIA_annotationboost_additional_task\n",
        "    )\n",
        "    from llm_utils import call_llm\n",
        "    print(\"Successfully imported unified annotation boost modules\")\n",
        "except ImportError as e:\n",
        "    print(f\"Note: Could not import unified modules: {str(e)}\")\n",
        "    print(\"Using original implementations from tools_function.py\")\n",
        "    # These will be provided by tools_function import\n",
        "\n",
        "# Setup configuration variables\n",
        "output_name = \"intestine_detailed\"\n",
        "model_name = \"google/gemini-2.5-flash-preview\"\n",
        "provider = \"openrouter\"\n",
        "tissue = \"large intestine\"\n",
        "species = \"human\"\n",
        "\n",
        "# Setup configuration variables for custermized api key (deepseek example)\n",
        "# Uncomment the following block to use DeepSeek as a custom provider\n",
        "# output_name = \"intestine_detailed\"\n",
        "# model_name = \"deepseek-chat\"\n",
        "# provider = \"https://api.deepseek.com\"\n",
        "# tissue = \"large intestine\"\n",
        "# species = \"human\"\n",
        "# api_key = \"sk-afb39114f1334ba486505d9425937d16\""
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load Example Data\n",
        "\n",
        "Load the built-in example datasets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Loaded unprocessed markers: (31313, 8)\n",
            "Columns: ['Unnamed: 0', 'p_val', 'avg_log2FC', 'pct.1', 'pct.2', 'p_val_adj', 'cluster', 'gene']\n",
            "Unique clusters: ['monocyte' 'plasma cell' 'cd8-positive, alpha-beta t cell'\n",
            " 'transit amplifying cell of large intestine'\n",
            " 'intestinal enteroendocrine cell' 'intestinal crypt stem cell']\n",
            "‚úì Loaded processed markers: (6, 3)\n",
            "‚úì Loaded subcluster results: (2023, 7)\n"
          ]
        }
      ],
      "source": [
        "# Load example marker data\n",
        "def load_example_data():\n",
        "    \"\"\"Load example marker datasets from the data directory\"\"\"\n",
        "    data_dir = Path(\"data\")\n",
        "    \n",
        "    # Load unprocessed markers (FindAllMarkers format)\n",
        "    unprocessed_path = data_dir / \"unprocessed.csv\"\n",
        "    if unprocessed_path.exists():\n",
        "        unprocessed_markers = pd.read_csv(unprocessed_path)\n",
        "        print(f\"‚úì Loaded unprocessed markers: {unprocessed_markers.shape}\")\n",
        "        print(f\"Columns: {list(unprocessed_markers.columns)}\")\n",
        "        print(f\"Unique clusters: {unprocessed_markers['cluster'].unique()}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Could not find {unprocessed_path}\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # Load processed markers (simple format)\n",
        "    processed_path = data_dir / \"processed.csv\"\n",
        "    if processed_path.exists():\n",
        "        processed_markers = pd.read_csv(processed_path)\n",
        "        print(f\"‚úì Loaded processed markers: {processed_markers.shape}\")\n",
        "    else:\n",
        "        processed_markers = None\n",
        "        print(f\"‚ö†Ô∏è Could not find {processed_path}\")\n",
        "    \n",
        "    # Load subcluster results\n",
        "    subcluster_path = data_dir / \"subcluster_results.csv\"\n",
        "    if subcluster_path.exists():\n",
        "        subcluster_results = pd.read_csv(subcluster_path)\n",
        "        print(f\"‚úì Loaded subcluster results: {subcluster_results.shape}\")\n",
        "    else:\n",
        "        subcluster_results = None\n",
        "        print(f\"‚ö†Ô∏è Could not find {subcluster_path}\")\n",
        "    \n",
        "    return unprocessed_markers, processed_markers, subcluster_results\n",
        "\n",
        "# Load the data\n",
        "unprocessed_markers, processed_markers, subcluster_results = load_example_data()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configure API Access\n",
        "\n",
        "Set up your API keys for the LLM providers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure API keys - Replace with your actual API keys\n",
        "def setup_api_keys():\n",
        "    \"\"\"Configure API keys for different providers\"\"\"\n",
        "    \n",
        "    # Option 1: OpenRouter (Recommended - provides access to multiple models)\n",
        "    openrouter_key = \"your-openrouter-api-key-here\"\n",
        "    if openrouter_key != \"your-openrouter-api-key-here\":\n",
        "        os.environ[\"OPENROUTER_API_KEY\"] = openrouter_key\n",
        "        print(\"‚úì OpenRouter API key configured\")\n",
        "    \n",
        "    # Option 2: OpenAI\n",
        "    openai_key = \"your-openai-api-key-here\"\n",
        "    if openai_key != \"your-openai-api-key-here\":\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "        print(\"‚úì OpenAI API key configured\")\n",
        "    \n",
        "    # Option 3: Anthropic\n",
        "    anthropic_key = \"your-anthropic-api-key-here\"\n",
        "    if anthropic_key != \"your-anthropic-api-key-here\":\n",
        "        os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_key\n",
        "        print(\"‚úì Anthropic API key configured\")\n",
        "    \n",
        "    # Option 4: Custom provider (e.g., DeepSeek)\n",
        "    custom_key = \"your-custom-api-key-here\"\n",
        "    custom_url = \"https://api.deepseek.com\"  # Example\n",
        "    if custom_key != \"your-custom-api-key-here\":\n",
        "        set_api_key(custom_key, provider=custom_url)\n",
        "        print(f\"‚úì Custom provider API key configured for {custom_url}\")\n",
        "    \n",
        "    print(\"\\nüìù Note: Replace the placeholder API keys above with your actual keys\")\n",
        "    print(\"Available providers: openrouter, openai, anthropic, or custom URLs\")\n",
        "\n",
        "setup_api_keys()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 1: Basic Single Cluster Annotation\n",
        "\n",
        "Test the core annotation function on a single cluster:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_single_annotation():\n",
        "    \"\"\"Test single cluster annotation\"\"\"\n",
        "    if unprocessed_markers is None:\n",
        "        print(\"‚ùå Cannot run test - no marker data loaded\")\n",
        "        return\n",
        "    \n",
        "    print(\"üß™ Testing single cluster annotation...\")\n",
        "    \n",
        "    # Get markers for one cluster (e.g., plasma cell)\n",
        "    cluster_name = \"plasma cell\"\n",
        "    cluster_markers = unprocessed_markers[unprocessed_markers['cluster'] == cluster_name]\n",
        "    \n",
        "    if cluster_markers.empty:\n",
        "        available_clusters = unprocessed_markers['cluster'].unique()\n",
        "        print(f\"‚ùå Cluster '{cluster_name}' not found. Available: {available_clusters}\")\n",
        "        cluster_name = available_clusters[0]\n",
        "        cluster_markers = unprocessed_markers[unprocessed_markers['cluster'] == cluster_name]\n",
        "        print(f\"Using '{cluster_name}' instead\")\n",
        "    \n",
        "    # Convert to simple format for testing\n",
        "    marker_list = cluster_markers.nlargest(20, 'avg_log2FC')[['gene', 'cluster']]\n",
        "    marker_list.columns = ['gene', 'cell_type']\n",
        "    \n",
        "    print(f\"Testing with {len(marker_list)} markers for {cluster_name}\")\n",
        "    print(f\"Top 5 markers: {list(marker_list['gene'].head())}\")\n",
        "    \n",
        "    try:\n",
        "        # Run annotation\n",
        "        result = runCASSIA(\n",
        "            tissue=\"large intestine\",\n",
        "            species=\"human\", \n",
        "            additional_info=None,\n",
        "            temperature=0.3,\n",
        "            marker_list=marker_list,\n",
        "            model=\"gpt-4o-mini\",  # Use a fast, cheap model for testing\n",
        "            provider=\"openai\",\n",
        "            validator_involvement=\"v1\"\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Single annotation test successful!\")\n",
        "        print(f\"Main cell type: {result.get('main_cell_type', 'N/A')}\")\n",
        "        print(f\"Sub cell types: {result.get('sub_cell_types', 'N/A')}\")\n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Single annotation test failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Run the test\n",
        "single_result = test_single_annotation()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 2: Batch Annotation\n",
        "\n",
        "Test batch processing of multiple clusters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing batch annotation...\n",
            "Processing FindAllMarkers-style dataframe with 8 columns\n",
            "Processed to 6 clusters with top markers\n",
            "Using 'cluster' as cell type column and 'markers' as gene column.\n",
            "All analyses completed. Results saved to 'local_test_batch'.\n",
            "Results saved to:\n",
            "  - Full results: local_test_batch_full.csv\n",
            "  - Summary: local_test_batch_summary.csv\n",
            "  - JSON: local_test_batch.json\n",
            "‚úÖ Batch annotation test successful!\n",
            "Results saved: 6 clusters annotated\n",
            "Columns: ['cell_type', 'main_cell_type', 'sub_cell_types', 'possible_mixed_cell_types', 'possible_tissues', 'iterations', 'num_markers', 'final_annotation', 'all_conversations']\n"
          ]
        }
      ],
      "source": [
        "def test_batch_annotation():\n",
        "    \"\"\"Test batch annotation of all clusters\"\"\"\n",
        "    if unprocessed_markers is None:\n",
        "        print(\"‚ùå Cannot run test - no marker data loaded\")\n",
        "        return\n",
        "    \n",
        "    print(\"üß™ Testing batch annotation...\")\n",
        "    \n",
        "    output_name = \"local_test_batch\"\n",
        "    \n",
        "    try:\n",
        "        # Run batch annotation\n",
        "        runCASSIA_batch(\n",
        "            marker=unprocessed_markers,\n",
        "            output_name=output_name,\n",
        "            model=\"gpt-4o-mini\",  # Fast model for testing\n",
        "            tissue=\"large intestine\",\n",
        "            species=\"human\",\n",
        "            max_workers=3,  # Reduced for testing\n",
        "            n_genes=30,  # Fewer genes for faster testing\n",
        "            additional_info=None,\n",
        "            provider=\"openai\",\n",
        "            validator_involvement=\"v1\"\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Batch annotation test successful!\")\n",
        "        \n",
        "        # Check output files\n",
        "        full_file = f\"{output_name}_full.csv\"\n",
        "        summary_file = f\"{output_name}_summary.csv\"\n",
        "        \n",
        "        if os.path.exists(full_file):\n",
        "            results_df = pd.read_csv(full_file)\n",
        "            print(f\"Results saved: {results_df.shape[0]} clusters annotated\")\n",
        "            print(f\"Columns: {list(results_df.columns)}\")\n",
        "            return results_df\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Output file {full_file} not found\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Batch annotation test failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Run the test\n",
        "batch_results = test_batch_annotation()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 3: Validator Parameter Testing\n",
        "\n",
        "Test the new validator_involvement parameter with both v0 (stricter) and v1 (moderate) validators:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_validator_levels():\n",
        "    \"\"\"Test different validator involvement levels\"\"\"\n",
        "    if unprocessed_markers is None:\n",
        "        print(\"‚ùå Cannot run test - no marker data loaded\")\n",
        "        return\n",
        "    \n",
        "    print(\"üß™ Testing validator involvement levels...\")\n",
        "    \n",
        "    # Test both v0 (stricter) and v1 (moderate) validators\n",
        "    validator_levels = [\"v0\", \"v1\"]\n",
        "    results = {}\n",
        "    \n",
        "    for validator in validator_levels:\n",
        "        print(f\"\\n--- Testing {validator} validator ---\")\n",
        "        output_name = f\"local_test_{validator}_validator\"\n",
        "        \n",
        "        try:\n",
        "            runCASSIA_batch(\n",
        "                marker=unprocessed_markers,\n",
        "                output_name=output_name,\n",
        "                model=\"gpt-4o-mini\",\n",
        "                tissue=\"large intestine\",\n",
        "                species=\"human\",\n",
        "                max_workers=2,  # Minimal for testing\n",
        "                n_genes=20,     # Even fewer genes\n",
        "                additional_info=None,\n",
        "                provider=\"openai\",\n",
        "                validator_involvement=validator\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úÖ {validator} validator test successful!\")\n",
        "            \n",
        "            # Check results\n",
        "            full_file = f\"{output_name}_full.csv\"\n",
        "            if os.path.exists(full_file):\n",
        "                df = pd.read_csv(full_file)\n",
        "                results[validator] = df\n",
        "                print(f\"Results: {df.shape[0]} clusters annotated\")\n",
        "                \n",
        "                # Show differences if both validators completed\n",
        "                if len(results) == 2:\n",
        "                    print(f\"\\nüìä Comparing validator results:\")\n",
        "                    v0_df = results.get(\"v0\")\n",
        "                    v1_df = results.get(\"v1\") \n",
        "                    if v0_df is not None and v1_df is not None:\n",
        "                        print(f\"v0 clusters: {v0_df.shape[0]}, v1 clusters: {v1_df.shape[0]}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {validator} validator test failed: {str(e)}\")\n",
        "            results[validator] = None\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the test\n",
        "validator_results = test_validator_levels()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 4: Quality Scoring\n",
        "\n",
        "Test the quality scoring functionality:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_quality_scoring():\n",
        "    \"\"\"Test quality scoring of annotation results\"\"\"\n",
        "    input_file = \"local_test_batch_full.csv\"\n",
        "    output_file = \"local_test_scored.csv\"\n",
        "    \n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"‚ùå Cannot run test - {input_file} not found\")\n",
        "        print(\"Run the batch annotation test first\")\n",
        "        return\n",
        "    \n",
        "    print(\"üß™ Testing quality scoring...\")\n",
        "    \n",
        "    try:\n",
        "        # Run quality scoring\n",
        "        runCASSIA_score_batch(\n",
        "            input_file=input_file,\n",
        "            output_file=output_file,\n",
        "            max_workers=3,\n",
        "            model=\"gpt-4o-mini\",\n",
        "            provider=\"openai\"\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Quality scoring test successful!\")\n",
        "        \n",
        "        # Check results\n",
        "        if os.path.exists(output_file):\n",
        "            scored_df = pd.read_csv(output_file)\n",
        "            print(f\"Scored results: {scored_df.shape[0]} clusters\")\n",
        "            if 'score' in scored_df.columns:\n",
        "                scores = scored_df['score'].dropna()\n",
        "                print(f\"Average score: {scores.mean():.1f}\")\n",
        "                print(f\"Score range: {scores.min()}-{scores.max()}\")\n",
        "            return scored_df\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Output file {output_file} not found\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Quality scoring test failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Run the test\n",
        "scoring_results = test_quality_scoring()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test Summary and Cleanup\n",
        "\n",
        "Review test results and provide guidance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_tests():\n",
        "    \"\"\"Summarize all test results\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üéØ CASSIA LOCAL TESTING SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Check which tests passed by looking for expected variables\n",
        "    tests = {\n",
        "        \"Single Annotation\": 'single_result' in globals() and single_result is not None,\n",
        "        \"Batch Annotation\": 'batch_results' in globals() and batch_results is not None,\n",
        "        \"Validator Testing\": 'validator_results' in globals() and validator_results is not None,\n",
        "        \"Quality Scoring\": 'scoring_results' in globals() and scoring_results is not None,\n",
        "    }\n",
        "    \n",
        "    for test_name, passed in tests.items():\n",
        "        status = \"‚úÖ PASSED\" if passed else \"‚ùå FAILED\"\n",
        "        print(f\"{test_name:<20}: {status}\")\n",
        "    \n",
        "    # Show generated files\n",
        "    print(\"\\nüìÅ Generated files:\")\n",
        "    test_files = [\n",
        "        \"local_test_batch_full.csv\",\n",
        "        \"local_test_batch_summary.csv\", \n",
        "        \"local_test_scored.csv\",\n",
        "        \"local_test_v0_validator_full.csv\",\n",
        "        \"local_test_v1_validator_full.csv\",\n",
        "    ]\n",
        "    \n",
        "    for file in test_files:\n",
        "        if os.path.exists(file):\n",
        "            size = os.path.getsize(file)\n",
        "            print(f\"  ‚úì {file} ({size} bytes)\")\n",
        "    \n",
        "    passed_count = sum(tests.values())\n",
        "    total_count = len(tests)\n",
        "    \n",
        "    print(f\"\\nüèÜ Overall: {passed_count}/{total_count} tests passed\")\n",
        "    \n",
        "    if passed_count == total_count:\n",
        "        print(\"üéâ All tests passed! CASSIA is working correctly.\")\n",
        "    elif passed_count >= total_count // 2:\n",
        "        print(\"‚ö†Ô∏è Most tests passed. Check failed tests above.\")\n",
        "    else:\n",
        "        print(\"üö® Multiple tests failed. Check API keys and error messages.\")\n",
        "    \n",
        "    # Provide next steps\n",
        "    print(\"\\nüöÄ Next Steps:\")\n",
        "    print(\"1. Set your actual API keys in the configuration cell\")\n",
        "    print(\"2. Try with your own marker data\")\n",
        "    print(\"3. Experiment with different models and parameters\")\n",
        "    print(\"4. Use runCASSIA_pipeline() for complete analysis\")\n",
        "    print(\"5. Explore advanced features like annotation boost\")\n",
        "\n",
        "# Run summary\n",
        "summarize_tests()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Optional: Clean Up Test Files\n",
        "\n",
        "Uncomment and run the cell below to clean up test files:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to clean up test files\n",
        "# test_files = [\n",
        "#     \"local_test_batch_full.csv\", \"local_test_batch_summary.csv\",\n",
        "#     \"local_test_scored.csv\", \"local_test_v0_validator_full.csv\",\n",
        "#     \"local_test_v0_validator_summary.csv\", \"local_test_v1_validator_full.csv\",\n",
        "#     \"local_test_v1_validator_summary.csv\"\n",
        "# ]\n",
        "# for file in test_files:\n",
        "#     if os.path.exists(file):\n",
        "#         os.remove(file)\n",
        "#         print(f\"Deleted {file}\")\n",
        "# print(\"Cleanup complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Using Your Own Data\n",
        "\n",
        "To use CASSIA with your own data:\n",
        "\n",
        "### 1. Prepare Your Marker Data\n",
        "\n",
        "**Option A: FindAllMarkers format (Recommended)**\n",
        "```python\n",
        "# Your data should have columns: gene, cluster, p_val, avg_log2FC, p_val_adj\n",
        "your_markers = pd.read_csv(\"your_findallmarkers.csv\")\n",
        "```\n",
        "\n",
        "**Option B: Simple format**\n",
        "```python\n",
        "# Your data should have columns: gene, cell_type\n",
        "your_markers = pd.DataFrame({\n",
        "    'gene': ['CD19', 'MS4A1', 'CD79A', 'CD79B'],\n",
        "    'cell_type': ['B cell', 'B cell', 'B cell', 'B cell']\n",
        "})\n",
        "```\n",
        "\n",
        "### 2. Run Analysis\n",
        "\n",
        "```python\n",
        "# For batch analysis\n",
        "runCASSIA_batch(\n",
        "    marker=your_markers,\n",
        "    output_name=\"my_analysis\",\n",
        "    model=\"gpt-4o\",  # Use better models for production\n",
        "    tissue=\"your_tissue\",\n",
        "    species=\"human\",  # or \"mouse\"\n",
        "    max_workers=6,\n",
        "    provider=\"openai\"\n",
        ")\n",
        "\n",
        "# For full pipeline (recommended)\n",
        "runCASSIA_pipeline(\n",
        "    output_file_name=\"my_pipeline_results\",\n",
        "    tissue=\"your_tissue\", \n",
        "    species=\"human\",\n",
        "    marker_path=your_markers,\n",
        "    annotation_model=\"gpt-4o\",\n",
        "    annotation_provider=\"openai\"\n",
        ")\n",
        "```\n",
        "\n",
        "### 3. Key Parameters\n",
        "\n",
        "- **model**: Choose based on accuracy vs speed (gpt-4o > gpt-4o-mini > gpt-3.5-turbo)\n",
        "- **provider**: \"openai\", \"anthropic\", \"openrouter\", or custom URLs\n",
        "- **validator_involvement**: \"v0\" (stricter) or \"v1\" (moderate)\n",
        "- **max_workers**: Adjust based on API rate limits\n",
        "- **n_genes**: Number of top genes per cluster (30-50 typical)\n",
        "\n",
        "### 4. Common Issues\n",
        "\n",
        "- **API Rate Limits**: Reduce max_workers or add delays\n",
        "- **Large Datasets**: Use smaller n_genes or process in batches  \n",
        "- **Custom Models**: Some providers require specific model names\n",
        "- **File Paths**: Use absolute paths for reliability\n",
        "\n",
        "### 5. Advanced Features\n",
        "\n",
        "Once basic analysis works, explore:\n",
        "- `runCASSIA_annotationboost()` for detailed cluster analysis\n",
        "- `runCASSIA_batch_n_times()` for uncertainty quantification\n",
        "- Custom prompts and additional information parameters\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cassia_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
