---
title: Quality Scoring
---


Quality scoring helps evaluate the reliability of cell type annotations. CASSIA provides automated scoring functionality through the `runCASSIA_score_batch` function, which analyzes the reasoning and evidence behind each annotation.

## Running Quality Scoring

### Basic Usage
```R
runCASSIA_score_batch(
    input_file = "my_annotation_full.csv",
    output_file = "my_annotation_scored.csv",
    max_workers = 4,
    model = "openai/gpt-5.1", # Recommended for accurate scoring
    provider = "openrouter",
    reasoning = "medium"  # Optional: "low", "medium", "high"
)
```

### Parameter Details

- **`input_file`** (character string): Path to the full annotation results CSV file (generated by `runCASSIA_batch`).
- **`output_file`** (character string): The name for the output scored results CSV file.
- **`max_workers`** (integer): Number of parallel scoring threads.
- **`model`** (character string): The LLM used for quality scoring. `openai/gpt-5.1` or `anthropic/claude-sonnet-4.5` is highly recommended for best accuracy.
- **`provider`** (character string): The API provider for the model (e.g., "openrouter").
- **`reasoning`** (character string, optional): Reasoning effort level ("low", "medium", "high"). Controls how much the model "thinks" before responding. Only supported by OpenAI GPT-5 series models (e.g., `gpt-5.1`). Via OpenRouter, no additional verification needed. Via direct OpenAI API, identity verification (KYC) is required.

### Output Format
The scored output file contains:
- Original annotation data
- Quality scores (0-100)
- Confidence metrics
- Detailed reasoning for scores

### Interpreting Scores

- **90-100**: High confidence, strong evidence
- **76-89**: Good confidence, adequate evidence
- **<75**: Low confidence, need to run through Annotation Boost Agent and Compare Agent

An HTML report is automatically generated at `{output_file}_report.html` containing all CASSIA outputs including structured results, conversation histories, and quality scores.
